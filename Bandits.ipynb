{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have a favourite coffee place in town? When you feel the need to replenish your caffeine levels, chances are you will choose to get a coffee from your favourite coffee place, as you are almost sure that you will get the best coffee or at least closer to your subjective preferences. This however means that you may be missing out on the delicious coffee served by this new place two blocks away. And if you were to try out all the coffee places one by one, the probability of having the worse coffee of your life would be pretty high! But then again, there’s a chance you’ll find an even better coffee brewer. \n",
    "\n",
    "The dilemma in our coffee tasting experiment arises from incomplete information. Put it differently, it is important to gather enough information in order to formulate the best overall strategy and then explore new actions. This will eventually lead to minimizing the overall bad experiences.\n",
    "\n",
    "A multi-armed bandit is a simplified form of this analogy. It is used to represent similar kinds of problems and finding a good strategy to solve them is already helping a lot of industries. In this Python notebook, we will explore some strategies on how to solve the multi armed bandit problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![robot](img/robot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af36d0302eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The general bandit algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we declare a general bandit algorithm that accepts various initialization, action selection and update strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit(bandit_inputs, seed=1):\n",
    "    \n",
    "    # Unpacking the bandit inputs:\n",
    "    k = bandit_inputs[\"k\"]\n",
    "    N = bandit_inputs[\"N\"]\n",
    "    q_star = bandit_inputs[\"q_star\"]\n",
    "    initialization = bandit_inputs[\"initialization\"]\n",
    "    action_selection = bandit_inputs[\"action_selection\"]\n",
    "    update_rule = bandit_inputs[\"update_rule\"]\n",
    "    \n",
    "    #Initializing rewards and regret\n",
    "    total_reward = 0\n",
    "    mean_reward = 0\n",
    "    mean_rewards = np.zeros(N)\n",
    "    regret = np.zeros(N)\n",
    "    \n",
    "    # Initialize the estimates\n",
    "    np.random.seed(seed)\n",
    "    estimates = initialization(k, q_star)\n",
    "    for t in range(1,N+1):\n",
    "        np.random.seed(seed*t) #we need this because random init changes the order later on\n",
    "        # Select a coffee shop\n",
    "        coffee_shop = action_selection(estimates, bandit_inputs, t)\n",
    "    \n",
    "        # Update action counter\n",
    "        bandit_inputs[\"k_t\"][coffee_shop] += 1\n",
    "    \n",
    "        # Get reward\n",
    "        reward = np.random.normal(q_star[coffee_shop], 1)\n",
    "        \n",
    "        # Update the total reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Calculate mean reward and regret\n",
    "        mean_reward = mean_reward + (reward - mean_reward) / t\n",
    "        mean_rewards[t-1] = mean_reward\n",
    "        #regret[t-1] = t * mean_rewards[np.argmax(mean_rewards[0:t])] - total_reward\n",
    "        regret[t-1] = np.max(q_star) - reward # True maximum! We do not know it but, why not? :)\n",
    "        #if regret[t-1]<0:\n",
    "            #print(\"what?\")\n",
    "        \n",
    "        estimates = update_rule(estimates, coffee_shop, reward, bandit_inputs, mean_reward)\n",
    "        \n",
    "    return estimates, total_reward, mean_rewards, regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Initialization strategies\n",
    "Using the q* values, we create initialization methods that implement zero, pessimistic, average, optimistic and random estimate initializations for the bandit algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate initialization with zeroes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ZeroInitialization(k, q_star):\n",
    "    # Zero initialization\n",
    "    return np.zeros(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pessimistic estimate initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MinInitialization(k, q_star):\n",
    "    # Min initialization\n",
    "    return np.ones(k) * np.min(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean estimate initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MeanInitialization(k, q_star):\n",
    "    # Mean initialization\n",
    "    return np.ones(k) * np.mean(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimistic estimate initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MaxInitialization(k, q_star):\n",
    "    # Max initialization\n",
    "    return np.ones(k) * np.max(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate initialization with random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomInitialization(k, q_star):\n",
    "    # Random initialization\n",
    "    return np.random.normal(0, 1, k)  #let's assume we know the distributions and take nice random initial values :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Action selection strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random action selection: \n",
    "This strategy selects an action completely at random without taking into account the learned estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_action_selection(estimates, bandit_inputs, t):\n",
    "    # Choose randomly (uniform)\n",
    "    return np.random.randint(bandit_inputs[\"k\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy action selection:\n",
    "This action selection strategy always selects the best possible action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy_action_selection(estimates, bandit_inputs, t):\n",
    "    # Choose randomly\n",
    "    return np.argmax(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ε-Greedy action selection:\n",
    "The implementation of the e-Greedy algorithm for the multi-armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_greedy_action_selection(estimates, bandit_inputs, t):\n",
    "    # Generate a random number\n",
    "    p = np.random.rand()\n",
    "    \n",
    "    # E-Greedy action selection\n",
    "    if p < bandit_inputs[\"epsilon\"]:\n",
    "        # Randomly select an action\n",
    "        return np.random.choice(bandit_inputs[\"k\"])\n",
    "    else:\n",
    "        # Take greedy action\n",
    "        return np.argmax(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ε-Greedy with epsion decay action selection:\n",
    "The implementation of the e-Greedy algorithm with epsilon decay for the multi-armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_decay_greedy_action_selection(estimates, bandit_inputs, t):\n",
    "\n",
    "    # Generate a random number\n",
    "    p = np.random.rand()\n",
    "\n",
    "    # E-Greedy action selection\n",
    "    if p < bandit_inputs[\"epsilon\"]*np.exp(-bandit_inputs[\"kappa\"]*t):\n",
    "        # Randomly select an action\n",
    "        action = np.random.choice(bandit_inputs[\"k\"])\n",
    "    else:\n",
    "        # Take greedy action\n",
    "        action = np.argmax(estimates)\n",
    "\n",
    "    #if bandit_inputs[\"epsilon\"] > bandit_inputs[\"min_epsilon\"]:\n",
    "        #bandit_inputs[\"epsilon\"] = bandit_inputs[\"e0\"]*np.exp(-bandit_inputs[\"kappa\"]*(np.sum(bandit_inputs[\"k_t\"])+1))\n",
    "        #if bandit_inputs[\"epsilon\"] < bandit_inputs[\"min_epsilon\"]:\n",
    "            #bandit_inputs[\"epsilon\"] = bandit_inputs[\"min_epsilon\"]\n",
    "            \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper Confidence Bound action selection:\n",
    "The implementation of the UCB algorithm for the multi armed bandit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ucb_action_selection(estimates, bandit_inputs, t):\n",
    "    t = np.sum(bandit_inputs[\"k_t\"]) + 1\n",
    "    # Select action according to UCB Criteria\n",
    "    return np.argmax(estimates + bandit_inputs[\"c\"] * np.sqrt(np.log(t) / (bandit_inputs[\"k_t\"]+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoftMax action selection:\n",
    "The implementation of the softmax bandit algorithm for the multi-armed bandit problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_action_selection(estimates, bandit_inputs, t):\n",
    "    # Softmax action selection\n",
    "    action = np.random.choice(bandit_inputs[\"k\"], 1, \\\n",
    "                              p=np.exp(estimates/bandit_inputs[\"tau\"]) / np.sum(np.exp(estimates/bandit_inputs[\"tau\"])))\n",
    "    return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Update strategies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True average update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_rule(estimates, coffee_shop, reward, bandit_inputs, mean_reward=None):\n",
    "    # Update the estimates\n",
    "    estimates[coffee_shop] = estimates[coffee_shop] + \\\n",
    "                            (reward - estimates[coffee_shop]) / bandit_inputs[\"k_t\"][coffee_shop]\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant learning rate update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_rule_constant_lr(estimates, coffee_shop, reward, bandit_inputs, mean_reward=None):\n",
    "    # Update the estimates\n",
    "    estimates[coffee_shop] = estimates[coffee_shop] + \\\n",
    "                            (reward - estimates[coffee_shop]) / (bandit_inputs[\"k_t\"][coffee_shop]*bandit_inputs[\"a\"])\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about decaying learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we declare the number of trials and arms of the multi-armed bandit problem. We also select a gaussian probability distribution to generate the true values of each arm and to simulate the randomness of the experiment. <br>After each experiment we print two plots to evaluate the performance of the algorithm. We print the loss that we incur due to time/rounds spent due to the learning, or else the regret, and the expected reward of the algorithm across the rounds of each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of coffee shops\n",
    "k = 10\n",
    "\n",
    "# Number of trials\n",
    "N = 10000\n",
    "\n",
    "# Declaration of the true q* values according to a gaussian distribution\n",
    "q_star = np.random.normal(0, 1, k)\n",
    "print(q_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting/printing:\n",
    "Definition of the print and plot of the results (We use this across the whole notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_bandits(N, estimates, total_reward, mean_rewards, regret):\n",
    "    print(\"Learned Estimates: {}\".format(estimates))\n",
    "    print(\"\")\n",
    "    print(\"Euclidean distance from q_star vector: {}\".format(np.linalg.norm(q_star-estimates)))\n",
    "    print(\"Total Reward: {}\".format(total_reward))\n",
    "    print(\"Mean Reward: {}\".format(mean_rewards[-1]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,N-1,N), np.cumsum(regret), 'b-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Total Regret',\n",
    "           title='Regret Curve')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,N-1,N), mean_rewards, 'b-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Average Reward',\n",
    "           title='Average Reward Curve')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the comp_plot, that plots the comparative plot of the various initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comp_plot(regret, pes_regret, avg_regret, opt_regret, rnd_regret, \\\n",
    "              mean_rewards, pes_mean_rewards, avg_mean_rewards, opt_mean_rewards, rnd_mean_rewards):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,N-1,N), np.cumsum(regret), 'b-')\n",
    "    ax.plot(np.linspace(0,N-1,N), np.cumsum(opt_regret), 'g-')\n",
    "    ax.plot(np.linspace(0,N-1,N), np.cumsum(avg_regret), 'r-')\n",
    "    ax.plot(np.linspace(0,N-1,N), np.cumsum(pes_regret), 'c-')\n",
    "    ax.plot(np.linspace(0,N-1,N), np.cumsum(rnd_regret), 'y-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Total Regret',\n",
    "           title='Regret Curve')\n",
    "    plt.legend(['Zero','Optimistic','Mean','Pesimistic','Random'])\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.linspace(0,N-1,N), mean_rewards, 'b-')\n",
    "    ax.plot(np.linspace(0,N-1,N), opt_mean_rewards, 'g-')\n",
    "    ax.plot(np.linspace(0,N-1,N), avg_mean_rewards, 'r-')\n",
    "    ax.plot(np.linspace(0,N-1,N), pes_mean_rewards, 'c-')\n",
    "    ax.plot(np.linspace(0,N-1,N), rnd_mean_rewards, 'y-')\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"Speed\")\n",
    "    ax.grid()\n",
    "    ax.set(xlabel='Time Steps', ylabel='Average Reward',\n",
    "           title='Average Reward Curve')\n",
    "    plt.legend(['Zero','Optimistic','Mean','Pesimistic','Random'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. The bandit algorithm with random action selection and true average update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "estimates_rand, total_reward_rand, mean_rewards_rand, regret_rand = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, estimates_rand, total_reward_rand, mean_rewards_rand, regret_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "min_estimates_rand, min_total_reward_rand, min_mean_rewards_rand, min_regret_rand = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, min_estimates_rand, min_total_reward_rand, min_mean_rewards_rand, min_regret_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "avg_estimates_rand, avg_total_reward_rand, avg_mean_rewards_rand, avg_regret_rand = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, avg_estimates_rand, avg_total_reward_rand, avg_mean_rewards_rand, avg_regret_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "max_estimates_rand, max_total_reward_rand, max_mean_rewards_rand, max_regret_rand = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, max_estimates_rand, max_total_reward_rand, max_mean_rewards_rand, max_regret_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Random\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : random_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "rnd_estimates_rand, rnd_total_reward_rand, rnd_mean_rewards_rand, rnd_regret_rand = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, rnd_estimates_rand, rnd_total_reward_rand, rnd_mean_rewards_rand, rnd_regret_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the random action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_rand, min_regret_rand, avg_regret_rand, max_regret_rand, rnd_regret_rand, \\\n",
    "          mean_rewards_rand, min_mean_rewards_rand, avg_mean_rewards_rand, max_mean_rewards_rand, rnd_mean_rewards_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. The bandit algorithm with greedy action selection and true average update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "estimates_greedy, total_reward_greedy, mean_rewards_greedy, regret_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))                 \n",
    "plot_bandits(N, estimates_greedy, total_reward_greedy, mean_rewards_greedy, regret_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "min_estimates_greedy, min_total_reward_greedy, min_mean_rewards_greedy, min_regret_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, min_estimates_greedy, min_total_reward_greedy, min_mean_rewards_greedy, min_regret_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "avg_estimates_greedy, avg_total_reward_greedy, avg_mean_rewards_greedy, avg_regret_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, avg_estimates_greedy, avg_total_reward_greedy, avg_mean_rewards_greedy, avg_regret_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "max_estimates_greedy, max_total_reward_greedy, max_mean_rewards_greedy, max_regret_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, max_estimates_greedy, max_total_reward_greedy, max_mean_rewards_greedy, max_regret_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "\n",
    "rnd_estimates_greedy = np.zeros(k)\n",
    "rnd_total_reward_greedy = 0\n",
    "rnd_mean_rewards_greedy = np.zeros(N)\n",
    "rnd_regret_greedy = np.zeros(N)\n",
    "\n",
    "for i in range(trials):\n",
    "    bandit_inputs = {\"k\"             : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "    i_estimates_greedy, i_total_reward_greedy, i_mean_rewards_greedy, i_regret_greedy = bandit(bandit_inputs, i)\n",
    "    rnd_estimates_greedy += i_estimates_greedy\n",
    "    rnd_total_reward_greedy += i_total_reward_greedy\n",
    "    rnd_mean_rewards_greedy += i_mean_rewards_greedy\n",
    "    rnd_regret_greedy += i_regret_greedy\n",
    "    #print(rnd_regret_greedy)\n",
    "\n",
    "rnd_estimates_greedy /= trials\n",
    "rnd_total_reward_greedy /= trials\n",
    "rnd_mean_rewards_greedy /= trials\n",
    "rnd_regret_greedy /= trials\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, rnd_estimates_greedy, rnd_total_reward_greedy, rnd_mean_rewards_greedy, rnd_regret_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the random action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_greedy, min_regret_greedy, avg_regret_greedy, max_regret_greedy, rnd_regret_greedy, \\\n",
    "          mean_rewards_greedy, min_mean_rewards_greedy, avg_mean_rewards_greedy, max_mean_rewards_greedy, \\\n",
    "          rnd_mean_rewards_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e-Greedy action selection strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out different epsilon values. (Especially the limit cases 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"epsilon\"           : 0.1, # Epsilon parameter \n",
    "                 \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "estimates_e_greedy, total_reward_e_greedy, mean_rewards_e_greedy, regret_e_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, estimates_e_greedy, total_reward_e_greedy, mean_rewards_e_greedy, regret_e_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"epsilon\"           : 0.1,\n",
    "                 \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "min_estimates_e_greedy, min_total_reward_e_greedy, min_mean_rewards_e_greedy, min_regret_e_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, min_estimates_e_greedy, min_total_reward_e_greedy, min_mean_rewards_e_greedy, min_regret_e_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"epsilon\"           : 0.1,\n",
    "                 \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "avg_estimates_e_greedy, avg_total_reward_e_greedy, avg_mean_rewards_e_greedy, avg_regret_e_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, avg_estimates_e_greedy, avg_total_reward_e_greedy, avg_mean_rewards_e_greedy, avg_regret_e_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"epsilon\"           : 0.1,\n",
    "                 \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "max_estimates_e_greedy, max_total_reward_e_greedy, max_mean_rewards_e_greedy, max_regret_e_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, max_estimates_e_greedy, max_total_reward_e_greedy, max_mean_rewards_e_greedy, max_regret_e_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "\n",
    "rnd_estimates_e_greedy = np.zeros(k)\n",
    "rnd_total_reward_e_greedy = 0\n",
    "rnd_mean_rewards_e_greedy = np.zeros(N)\n",
    "rnd_regret_e_greedy = np.zeros(N)\n",
    "\n",
    "for i in range(trials):\n",
    "    bandit_inputs = {\"k\"             : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"epsilon\"           : 0.1,\n",
    "                 \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "    i_estimates_e_greedy, i_total_reward_e_greedy, i_mean_rewards_e_greedy, i_regret_e_greedy = bandit(bandit_inputs)\n",
    "    rnd_estimates_e_greedy += i_estimates_e_greedy\n",
    "    rnd_total_reward_e_greedy += i_total_reward_e_greedy\n",
    "    rnd_mean_rewards_e_greedy += i_mean_rewards_e_greedy\n",
    "    rnd_regret_e_greedy += i_regret_e_greedy\n",
    "\n",
    "rnd_estimates_e_greedy /= trials\n",
    "rnd_total_reward_e_greedy /= trials\n",
    "rnd_mean_rewards_e_greedy /= trials\n",
    "rnd_regret_e_greedy /= trials\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, rnd_estimates_e_greedy, rnd_total_reward_e_greedy, rnd_mean_rewards_e_greedy, rnd_regret_e_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the e-greedy action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_e_greedy, min_regret_e_greedy, avg_regret_e_greedy, max_regret_e_greedy, rnd_regret_e_greedy, \\\n",
    "          mean_rewards_e_greedy, min_mean_rewards_e_greedy, avg_mean_rewards_e_greedy, max_mean_rewards_e_greedy, \\\n",
    "          rnd_mean_rewards_e_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e-Greedy with epsilon decay action selection strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"kappa\"             : 0.001, # Decay coefficient\n",
    "                 \"epsilon\"           : 0.1,  # The epsilon parameter. (try with 0.01 as well--will it get stuck to lockal minima?)\n",
    "                 \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "estimates_e_decay_greedy, total_reward_e_decay_greedy, mean_rewards_e_decay_greedy, regret_e_decay_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, estimates_e_decay_greedy, total_reward_e_decay_greedy, mean_rewards_e_decay_greedy, regret_e_decay_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out different epsilon, min_epsilon and kappa values. (Especially the limit cases 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"kappa\"             : 0.001, # Decay coefficient\n",
    "                 \"epsilon\"           : 0.1,  # The epsilon parameter.\n",
    "                 \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "min_estimates_e_decay_greedy, min_total_reward_e_decay_greedy, min_mean_rewards_e_decay_greedy, min_regret_e_decay_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))                 \n",
    "plot_bandits(N, min_estimates_e_decay_greedy, min_total_reward_e_decay_greedy, min_mean_rewards_e_decay_greedy, min_regret_e_decay_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"kappa\"             : 0.001, # Decay coefficient\n",
    "                 \"epsilon\"           : 0.1,  # The epsilon parameter.\n",
    "                 \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "avg_estimates_e_decay_greedy, avg_total_reward_e_decay_greedy, avg_mean_rewards_e_decay_greedy, avg_regret_e_decay_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, avg_estimates_e_decay_greedy, avg_total_reward_e_decay_greedy, avg_mean_rewards_e_decay_greedy, avg_regret_e_decay_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"kappa\"             : 0.001, # Decay coefficient\n",
    "                 \"epsilon\"           : 0.1,  # The epsilon parameter.\n",
    "                 \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "max_estimates_e_decay_greedy, max_total_reward_e_decay_greedy, max_mean_rewards_e_decay_greedy, max_regret_e_decay_greedy = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, max_estimates_e_decay_greedy, max_total_reward_e_decay_greedy, max_mean_rewards_e_decay_greedy, max_regret_e_decay_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "\n",
    "rnd_estimates_e_decay_greedy = np.zeros(k)\n",
    "rnd_total_reward_e_decay_greedy = 0\n",
    "rnd_mean_rewards_e_decay_greedy = np.zeros(N)\n",
    "rnd_regret_e_decay_greedy = np.zeros(N)\n",
    "\n",
    "for i in range(trials):\n",
    "    bandit_inputs = {\"k\"             : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"kappa\"             : 0.001, # Decay coefficient\n",
    "                 \"epsilon\"           : 0.1,  # The epsilon parameter.\n",
    "                 \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : e_decay_greedy_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "    i_estimates_e_decay_greedy, i_total_reward_e_decay_greedy, i_mean_rewards_e_decay_greedy, i_regret_e_decay_greedy = bandit(bandit_inputs)\n",
    "    rnd_estimates_e_decay_greedy += i_estimates_e_decay_greedy\n",
    "    rnd_total_reward_e_decay_greedy += i_total_reward_e_decay_greedy\n",
    "    rnd_mean_rewards_e_decay_greedy += i_mean_rewards_e_decay_greedy\n",
    "    rnd_regret_e_decay_greedy += i_regret_e_decay_greedy\n",
    "\n",
    "rnd_estimates_e_decay_greedy /= trials\n",
    "rnd_total_reward_e_decay_greedy /= trials\n",
    "rnd_mean_rewards_e_decay_greedy /= trials\n",
    "rnd_regret_e_decay_greedy /= trials\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, rnd_estimates_e_decay_greedy, rnd_total_reward_e_decay_greedy, rnd_mean_rewards_e_decay_greedy, rnd_regret_e_decay_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the e-Greedy with epsilon decay action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_e_decay_greedy, min_regret_e_decay_greedy, avg_regret_e_decay_greedy,\\\n",
    "          max_regret_e_decay_greedy, rnd_regret_e_decay_greedy, mean_rewards_e_decay_greedy,\\\n",
    "          min_mean_rewards_e_decay_greedy, avg_mean_rewards_e_decay_greedy,\\\n",
    "          max_mean_rewards_e_decay_greedy, rnd_mean_rewards_e_decay_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound action selection strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out different c values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"c\"                 : 2., # The c parameter\n",
    "                 \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "estimates_ucb, total_reward_ucb, mean_rewards_ucb, regret_ucb = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, estimates_ucb, total_reward_ucb, mean_rewards_ucb, regret_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"c\"                 : 2., # The c parameter\n",
    "                 \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "min_estimates_ucb, min_total_reward_ucb, min_mean_rewards_ucb, min_regret_ucb = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, min_estimates_ucb, min_total_reward_ucb, min_mean_rewards_ucb, min_regret_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"c\"                 : 2., # The c parameter\n",
    "                 \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "avg_estimates_ucb, avg_total_reward_ucb, avg_mean_rewards_ucb, avg_regret_ucb = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, avg_estimates_ucb, avg_total_reward_ucb, avg_mean_rewards_ucb, avg_regret_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"c\"                 : 2., # The c parameter\n",
    "                 \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "max_estimates_ucb, max_total_reward_ucb, max_mean_rewards_ucb, max_regret_ucb = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, max_estimates_ucb, max_total_reward_ucb, max_mean_rewards_ucb, max_regret_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "\n",
    "rnd_estimates_ucb = np.zeros(k)\n",
    "rnd_total_reward_ucb = 0\n",
    "rnd_mean_rewards_ucb = np.zeros(N)\n",
    "rnd_regret_ucb = np.zeros(N)\n",
    "\n",
    "for i in range(trials):\n",
    "    bandit_inputs = {\"k\"             : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"c\"                 : 2., # The c parameter\n",
    "                 \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : ucb_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "    i_estimates_ucb, i_total_reward_ucb, i_mean_rewards_ucb, i_regret_ucb = bandit(bandit_inputs)\n",
    "    rnd_estimates_ucb += i_estimates_ucb\n",
    "    rnd_total_reward_ucb += i_total_reward_ucb\n",
    "    rnd_mean_rewards_ucb += i_mean_rewards_ucb\n",
    "    rnd_regret_ucb += i_regret_ucb\n",
    "\n",
    "rnd_estimates_ucb /= trials\n",
    "rnd_total_reward_ucb /= trials\n",
    "rnd_mean_rewards_ucb /= trials\n",
    "rnd_regret_ucb /= trials\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, rnd_estimates_ucb, rnd_total_reward_ucb, rnd_mean_rewards_ucb, rnd_regret_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the ucb action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_ucb, min_regret_ucb, avg_regret_ucb, max_regret_ucb, rnd_regret_ucb, \\\n",
    "          mean_rewards_ucb, min_mean_rewards_ucb, avg_mean_rewards_ucb, max_mean_rewards_ucb, \\\n",
    "          rnd_mean_rewards_ucb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax action selection strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out different tau values. What happens very close to 0 and with very high tau values? Try to find a good tau parameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with zero values:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"tau\"               : 0.5, # The tau parameter\n",
    "                 \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "estimates_sm, total_reward_sm, mean_rewards_sm, regret_sm = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, estimates_sm, total_reward_sm, mean_rewards_sm, regret_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Pessimistic:\n",
    "What do you think the expected reward is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"tau\"               : 0.5, # The tau parameter\n",
    "                 \"initialization\"    : MinInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "min_estimates_sm, min_total_reward_sm, min_mean_rewards_sm, min_regret_sm = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, min_estimates_sm, min_total_reward_sm, min_mean_rewards_sm, min_regret_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Mean:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"tau\"               : 0.5, # The tau parameter\n",
    "                 \"initialization\"    : MeanInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "avg_estimates_sm, avg_total_reward_sm, avg_mean_rewards_sm, avg_regret_sm = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, avg_estimates_sm, avg_total_reward_sm, avg_mean_rewards_sm, avg_regret_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Optimistic:\n",
    "What do you think the expected reward is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"tau\"               : 0.5, # The tau parameter\n",
    "                 \"initialization\"    : MaxInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "\n",
    "max_estimates_sm, max_total_reward_sm, max_mean_rewards_sm, max_regret_sm = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, max_estimates_sm, max_total_reward_sm, max_mean_rewards_sm, max_regret_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing with Random\n",
    "What do you think the expected reward is?\n",
    "(we will use the average of 100 random initializations, so that the results are significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "\n",
    "rnd_estimates_sm = np.zeros(k)\n",
    "rnd_total_reward_sm = 0\n",
    "rnd_mean_rewards_sm = np.zeros(N)\n",
    "rnd_regret_sm = np.zeros(N)\n",
    "\n",
    "for i in range(trials):\n",
    "    bandit_inputs = {\"k\"             : k, # Number of coffee shops\n",
    "                 \"k_t\"               : np.zeros(k), # The coffee shop selection counter\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"tau\"               : 0.5, # The tau parameter\n",
    "                 \"initialization\"    : RandomInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : softmax_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : update_rule # The update rule\n",
    "                 }\n",
    "    i_estimates_sm, i_total_reward_sm, i_mean_rewards_sm, i_regret_sm = bandit(bandit_inputs)\n",
    "    rnd_estimates_sm += i_estimates_sm\n",
    "    rnd_total_reward_sm += i_total_reward_sm\n",
    "    rnd_mean_rewards_sm += i_mean_rewards_sm\n",
    "    rnd_regret_sm += i_regret_sm\n",
    "\n",
    "rnd_estimates_sm /= trials\n",
    "rnd_total_reward_sm /= trials\n",
    "rnd_mean_rewards_sm /= trials\n",
    "rnd_regret_sm /= trials\n",
    "\n",
    "print(\"True q values:{}\".format(q_star))\n",
    "plot_bandits(N, rnd_estimates_sm, rnd_total_reward_sm, rnd_mean_rewards_sm, rnd_regret_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a comparison between the different initialization strategies for the softmax action selection strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_plot(regret_sm, min_regret_sm, avg_regret_sm, max_regret_sm, rnd_regret_sm, \\\n",
    "          mean_rewards_sm, min_mean_rewards_sm, avg_mean_rewards_sm, max_mean_rewards_sm, \\\n",
    "          rnd_mean_rewards_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement the gradient bandits action selection and update rule methods yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the gradient bandits with zeros initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_inputs = {\"k\"                 : k, # Number of coffee shops\n",
    "                 \"lr\"                : 4., # Learning rate\n",
    "                 \"bias\"              : True, # Bias (mean values)\n",
    "                 \"N\"                 : N, # Number of trials\n",
    "                 \"k_t\"               : np.zeros(k), # NOT NEEDED IN GRADIENT--- KEPT HERE TO HAVE THE SAME GENERAL ALGO\n",
    "                 \"q_star\"            : q_star, # The true unknown rewards\n",
    "                 \"initialization\"    : ZeroInitialization, # Initialization strategy\n",
    "                 \"action_selection\"  : gradient_bandit_action_selection, # Action selection strategy\n",
    "                 \"update_rule\"       : gradient_bandit_update_rule # The update rule\n",
    "                }\n",
    "\n",
    "estimates_grd, total_reward_grd, mean_rewards_grd, regret_grd = bandit(bandit_inputs)\n",
    "print(\"True q values:{}\".format(q_star))                 \n",
    "plot_bandits(N, estimates_grd, total_reward_grd, mean_rewards_grd, regret_grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Will gradient bandits have a different result using other initializations?\n",
    "Run the gradient bandits using different initializations to find out if you were correct:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Bandit Algorithms\n",
    "Compare the bandit algorithms through plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results of all the algorithms ###\n",
    "##############################################\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(0,N-1,N), np.cumsum(regret_rand), 'b-')\n",
    "ax.plot(np.linspace(0,N-1,N), np.cumsum(regret_greedy), 'g-')\n",
    "ax.plot(np.linspace(0,N-1,N), np.cumsum(regret_e_greedy), 'r-')\n",
    "ax.plot(np.linspace(0,N-1,N), np.cumsum(regret_e_decay_greedy), 'c-')\n",
    "ax.plot(np.linspace(0,N-1,N), np.cumsum(regret_ucb), 'm-')\n",
    "ax.plot(np.linspace(0,N-1,N), np.cumsum(regret_sm), 'y-')\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Speed\")\n",
    "ax.grid()\n",
    "ax.set(xlabel='Time Steps', ylabel='Total Regret',\n",
    "       title='Regret Curve')\n",
    "plt.legend(['Random','Greedy','e-Greedy','e-Greedy with e decay','UCB','Softmax'])\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(0,N-1,N), mean_rewards_rand, 'b-')\n",
    "ax.plot(np.linspace(0,N-1,N), mean_rewards_greedy, 'g-')\n",
    "ax.plot(np.linspace(0,N-1,N), mean_rewards_e_greedy, 'r-')\n",
    "ax.plot(np.linspace(0,N-1,N), mean_rewards_e_decay_greedy, 'c-')\n",
    "ax.plot(np.linspace(0,N-1,N), mean_rewards_ucb, 'm-')\n",
    "ax.plot(np.linspace(0,N-1,N), mean_rewards_sm, 'y-')\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Speed\")\n",
    "ax.grid()\n",
    "ax.set(xlabel='Time Steps', ylabel='Average Reward',\n",
    "       title='Average Reward Curve')\n",
    "plt.legend(['Random','Greedy','e-Greedy','e-Greedy with e decay','UCB','Softmax'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a bandit of your own liking!\n",
    "(A hybrid bandit maybe? play with constant step size? compare different greedy ones? different gradient-based ones?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
